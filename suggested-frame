Below is the consolidated and updated **Data Mining Project** for the Master 1 MLDM program at Université Jean Monnet for 2025, using the **Sudden Cardiac Death Holter Database.csv** from Kaggle (https://www.kaggle.com/datasets/sadmansakib7/ecg-arrhythmia-classification-dataset/data?select=Sudden+Cardiac+Death+Holter+Database.csv). This version integrates all previous modifications, including the shift to the new dataset, the 2025 timeline, the recommendation to use only PostgreSQL as the database, and the emphasis on data mining, knowledge discovery, Git version control, and comparisons. An Pham’s expertise in data mining, databases (including PostgreSQL), and healthcare AI is leveraged throughout. The report and coding align with the course guidelines, focusing on the six-phase data mining process.

---

### Project Description
**Project:** Analyzing the Sudden Cardiac Death Holter Database to discover ECG patterns for predicting sudden cardiac death (SCD) risk and segmenting patients, supporting advanced cardiac care.  
- **Dataset:** “Sudden Cardiac Death Holter Database.csv” from Kaggle (https://www.kaggle.com/datasets/sadmansakib7/ecg-arrhythmia-classification-dataset/data?select=Sudden+Cardiac+Death+Holter+Database.csv), containing ECG signal features and SCD risk labels from the Sudden Cardiac Death Holter Database.  
- **Problem:** Identify hidden ECG patterns (e.g., SCD risk factors, patient clusters) to improve early risk prediction and personalized treatment, aligning with An’s interest in medical applications and societal impact (e.g., healthcare innovation).  
- **Focus:** Use data mining techniques (classification with Random Forest, clustering with K-Means and DBSCAN), store and query data in PostgreSQL (chosen for its structured data handling and An’s expertise), and compare data mining methods, with Git for version control.

---

### Report Structure (6 Pages)

#### 1. Problem Understanding (≈1 page)
- **Objective:** Define the real-world medical problem and justify its relevance.
- **What to Do:**
  - **Describe the Problem:** Focus on predicting sudden cardiac death (SCD) risk using ECG data, a critical issue with high mortality rates (e.g., over 300,000 annual deaths in the US).
  - **Objectives:** Aim to answer: Which ECG features predict SCD risk? Can patients be segmented by risk profiles?
  - **Justification:** Highlight healthcare significance and An’s motivation (e.g., interest in medical AI and societal good from his explainable AI research).
  - **Course Alignment:** Relate to finding “amazing knowledge” (e.g., novel SCD risk patterns) in large datasets.
- **Example Content:**  
  Predicting sudden cardiac death (SCD) risk using ECG data is a critical healthcare challenge, with over 300,000 annual deaths in the US alone. As a Master’s student in Machine Learning with a passion for medical applications, inspired by my research in explainable AI, I aim to uncover hidden ECG patterns in the Sudden Cardiac Death Holter Database. This project seeks to identify predictive features and patient segments for early risk detection, supporting personalized cardiac care, aligning with the course’s goal of extracting valuable, unexpected knowledge from large datasets.

#### 2. Data Understanding (≈1 page)
- **Objective:** Describe the dataset, perform exploratory data analysis (EDA), and introduce PostgreSQL integration.
- **What to Do:**
  - **Source and Access:** The dataset is from Kaggle (https://www.kaggle.com/datasets/sadmansakib7/ecg-arrhythmia-classification-dataset/data?select=Sudden+Cardiac+Death+Holter+Database.csv), derived from the Sudden Cardiac Death Holter Database.
  - **Dataset Details:** 
    - Observations: Approximately 85,000 ECG records (adjust after download).
    - Variables: 15 (e.g., Age, Sex, QRS_Duration, P_R, Q_T, T_Interval, Heart_Rate, ST_Segment, SCD_Risk_Label; confirm after download).
    - File size: ~20 MB CSV (estimate; adjust after download).
  - **EDA in R:**
    - Load data: `df <- read.csv("Sudden_Cardiac_Death_Holter_Database.csv")`.
    - Descriptive statistics: `summary(df)` for means (e.g., Heart_Rate), `table(df$SCD_Risk_Label)` for class distribution.
    - Visualization: Use `ggplot2` for histograms (`ggplot(df, aes(x=Heart_Rate)) + geom_histogram()`), scatter plots (`ggplot(df, aes(x=QRS_Duration, y=Heart_Rate, color=SCD_Risk_Label)) + geom_point()`), and box plots (`ggplot(df, aes(y=QRS_Duration)) + geom_boxplot()`).
  - **Initial Insights:** Note imbalanced SCD_Risk_Label (e.g., 80% low risk vs. 20% high risk), missing ST_Segment values (5%), and correlation between QRS_Duration and Heart_Rate.
  - **Database Integration (Initial Look):** Plan to store data in PostgreSQL for structured querying, leveraging its relational capabilities.
  - **Course Alignment:** Link to the “Data Understanding” phase, emphasizing EDA’s role in identifying initial patterns.
- **Example Content:**  
  The Sudden Cardiac Death Holter Database, sourced from Kaggle (link), contains approximately 85,000 ECG records with 15 variables like Heart_Rate and SCD_Risk_Label (20 MB CSV), derived from Holter monitoring data. Using R, I computed summary statistics (e.g., Heart_Rate mean=72 bpm) and visualized distributions with ggplot2, noting 5% missing ST_Segment values, 80% low-risk labels, and a correlation between QRS_Duration and Heart_Rate. I plan to store data in PostgreSQL for efficient querying, aligning with the course’s Data Understanding phase.

#### 3. Data Preparation (≈1 page)
- **Objective:** Clean, transform, and store data in PostgreSQL, using Git for version control.
- **What to Do:**
  - **Cleaning:** Drop rows with missing ST_Segment (`df <- df[!is.na(df$ST_Segment), ]`), remove outliers (e.g., Heart_Rate > 150), oversample high-risk SCD labels.
  - **Transformation:** Create features (e.g., `df$ST_Anomaly <- ifelse(df$ST_Segment > 2, 1, 0)`), normalize numerical features (e.g., QRS_Duration with `scale()`), encode SCD_Risk_Label as categorical.
  - **Database Implementation:**
    - **PostgreSQL Only:** Create table (`CREATE TABLE scd_data (RecordID SERIAL PRIMARY KEY, Age INT, Sex TEXT, QRS_Duration FLOAT, P_R FLOAT, Q_T FLOAT, T_Interval FLOAT, Heart_Rate FLOAT, ST_Segment FLOAT, SCD_Risk_Label TEXT)`), load data using `RPostgreSQL` (`library(RPostgreSQL); conn <- dbConnect(PostgreSQL(), ...); dbWriteTable(conn, "scd_data", df)`), query with SQL (`SELECT SCD_Risk_Label, AVG(Heart_Rate) FROM scd_data GROUP BY SCD_Risk_Label`). Optimize with indexes (e.g., `CREATE INDEX idx_scd_risk ON scd_data(SCD_Risk_Label)`).
    - Compare query performance (e.g., indexed vs. non-indexed queries).
  - **Splitting:** Create 80/20 training/testing sets (`set.seed(123); train_idx <- sample(1:nrow(df), 0.8*nrow(df))`).
  - **Git:** Commit changes (`git add .`, `git commit -m "Data preparation completed"`, `git push`).
  - **Course Alignment:** Link to the “Data Preparation” phase, mentioning handling missing values and transformations.
- **Example Content:**  
  I cleaned the dataset in R by removing 5% rows with missing ST_Segment and outliers (Heart_Rate > 150), oversampling high-risk SCD labels. Features like ST_Anomaly were added, and numerical variables were normalized using scale(). Data was stored in PostgreSQL, with a query to average Heart_Rate by SCD_Risk_Label taking 0.4s, improved to 0.3s with an index. I split data into 80/20 sets and tracked changes with Git, aligning with the course’s Data Preparation phase.

#### 4. Modeling (≈1 page)
- **Objective:** Apply data mining techniques, integrate PostgreSQL, and compare methods.
- **What to Do:**
  - **Techniques:**
    - **Classification:** Use Random Forest (`randomForest` package) to predict SCD_Risk_Label.
    - **Clustering:** Apply K-Means and DBSCAN (`cluster` package) to segment patients by ECG patterns.
  - **Implementation in R:**
    - Random Forest: `library(randomForest); rf_model <- randomForest(SCD_Risk_Label ~ ., data=df[train_idx, ], ntree=100)`.
    - K-Means: `customer_features <- df[train_idx, c("QRS_Duration", "Heart_Rate")]; kmeans_result <- kmeans(customer_features, centers=3)`.
    - DBSCAN: `library(cluster); dbscan_result <- dbscan(customer_features, eps=0.5, minPts=5)`.
  - **Database Interaction:** Query training data from PostgreSQL (`SELECT * FROM scd_data WHERE RecordID IN (training_ids)`), store predictions in a new table (`CREATE TABLE predictions (RecordID INT, Predicted_Label TEXT); dbWriteTable(conn, "predictions", pred_df)`).
  - **Justification:** Random Forest for robust classification with mixed features, K-Means for defined clusters, DBSCAN for noise handling in ECG data.
  - **Course Alignment:** Reference course techniques (classification, clustering).
- **Example Content:**  
  I used Random Forest to predict SCD_Risk_Label, K-Means (3 clusters) and DBSCAN (eps=0.5) for patient segmentation, implemented in R with randomForest and cluster packages. Training data was queried from PostgreSQL, and predictions were stored in a PostgreSQL predictions table. Random Forest was chosen for its feature handling, while K-Means and DBSCAN were selected to compare structured vs. noise-aware clustering, aligning with course techniques.

#### 5. Evaluation (≈1 page)
- **Objective:** Evaluate results, compare techniques and SQL optimizations, and highlight “amazing knowledge.”
- **What to Do:**
  - **Results:**
    - Random Forest: Accuracy=85%, precision/recall=84%/83%.
    - Clustering: K-Means silhouette score=0.58, DBSCAN=0.48.
  - **Amazing Knowledge:** Unexpected finding (e.g., “Patients with elevated ST_Segment and normal Heart_Rate form a distinct cluster, indicating a new SCD risk profile”).
  - **Comparisons:**
    - Techniques: Random Forest outperformed baseline (e.g., 70% accuracy with k-NN), K-Means outperformed DBSCAN (higher silhouette score).
    - SQL Optimizations: Indexed queries (0.3s) vs. non-indexed (0.4s), showing indexing benefits.
  - **Computational Time:** Random Forest: 18s, K-Means: 7s, DBSCAN: 11s (using `system.time()`).
  - **Course Alignment:** Link to course evaluation methods (accuracy, silhouette scores).
- **Example Content:**  
  Random Forest achieved 85% accuracy (precision=84%, recall=83%), outperforming a k-NN baseline (70%). K-Means (silhouette=0.58) outperformed DBSCAN (silhouette=0.48), revealing clearer patient segments. An unexpected finding was a cluster with elevated ST_Segment and normal Heart_Rate, suggesting a new SCD risk profile. Indexed PostgreSQL queries took 0.3s vs. 0.4s without indexes. Times were 18s (Random Forest), 7s (K-Means), and 11s (DBSCAN), measured with system.time, aligning with course evaluation methods.

#### 6. Deployment (≈0.5 page)
- **Objective:** Discuss real-world applications and PostgreSQL’s role in deployment.
- **What to Do:**
  - **Application:** Use Random Forest for a real-time SCD risk detection tool, clustering for patient risk profiling.
  - **Deployment Options:** Suggest integration into a cardiac monitoring system, using PostgreSQL for structured data.
  - **Database Role:** Emphasize PostgreSQL’s efficiency for storing patient records and serving predictions.
  - **Course Alignment:** Reference the “Deployment” phase.
- **Example Content:**  
  The Random Forest model can be deployed in a cardiac monitoring system for real-time SCD risk detection, while K-Means segments can guide patient risk profiling. PostgreSQL can manage structured patient data and serve predictions efficiently, aligning with the course’s Deployment phase.

#### References (≈0.5 page)
- **What to Do:** Cite sources in ACM style.
- **Example Content:**  
  [1] Kaggle. 2023. ECG Arrhythmia Classification Dataset. URL: <link>.  
  [2] Liaw, A., Wiener, M. 2002. Classification and Regression by randomForest. R News 2(3), 18-22.  
  [3] R Core Team. 2023. R: A Language and Environment for Statistical Computing. URL: <link>.

---

### Coding and Storage Requirements
- **GitHub Repository:**
  - Create `MLDM-DataMining-SCD-2025` on GitHub.
  - Structure:
    - `/data/`: `Sudden_Cardiac_Death_Holter_Database.csv`.
    - `/code/`: Scripts (`eda.R`, `preparation.R`, `modeling.R`).
    - `/report/`: `report.pdf`.
    - `README.md`: Instructions to connect to PostgreSQL.
  - Git Commands: `git init`, `git add .`, `git commit -m "EDA completed"`, `git push`.
- **Database Integration:**
  - **PostgreSQL Only:** Store ECG records, query with SQL (e.g., `SELECT SCD_Risk_Label, AVG(Heart_Rate) FROM scd_data GROUP BY SCD_Risk_Label`), optimize with indexes (e.g., `CREATE INDEX idx_scd_risk ON scd_data(SCD_Risk_Label)`).
- **Coding Tasks in R:**
  - Dependencies: Install `RPostgreSQL`, `randomForest`, `cluster`, `ggplot2`.
  - EDA: `summary()`, `ggplot()`.
  - Modeling: Random Forest (`randomForest`), K-Means/DBSCAN (`cluster`).
- **Example README.md:**  
  ```
  # MLDM Data Mining Project 2025
  ## Setup
  1. Clone: `git clone <link>`
  2. Install R packages: `install.packages(c("RPostgreSQL", "randomForest", "cluster", "ggplot2"))`
  3. Setup PostgreSQL: Configure connection (see code/db_setup.R)
  4. Run: `source("code/main.R")`
  ## Dataset
  - `data/Sudden_Cardiac_Death_Holter_Database.csv`
  ## Report
  - `report/report.pdf`
  ```

---

### Submission
- **Deadline:** Submit on Moodle by March 31, 2025, 11:59 PM CET.
- **Plagiarism:** Cite all sources to avoid issues.

---

### Rationale for PostgreSQL
- **Fit:** The dataset’s structured nature (fixed columns) and need for aggregations (e.g., average Heart_Rate) suit PostgreSQL’s relational model.
- **An’s Expertise:** An’s CV highlights PostgreSQL experience, ensuring efficient implementation.
- **Course Alignment:** Supports data mining phases with robust querying, enabling “amazing knowledge” discovery.
- **Trade-Offs:** Cassandra and MongoDB are better for unstructured or scalable data, but the current project’s size and structure favor PostgreSQL.

This consolidated version integrates all updates, focusing on the Sudden Cardiac Death Holter Database, the 2025 timeline, and PostgreSQL usage, while meeting course requirements. Adjust dataset details (e.g., variables, size) after downloading, and let me know if further refinements are needed!
